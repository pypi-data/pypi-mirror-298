{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Conditional sampling using residuals vs sampling Random Forest\n\nTo deploy the Conditional Permutation Importance (CPI),\n:footcite:t:`Chamma_NeurIPS2023` described two main approaches for the\nconditional scheme: 1) Instead of directly permuting the variable of interest as\nin the Permutation Feature Importance (PFI), the residuals of the prediction of\nthe variable of interest x_j based on the remaining variables is first computed\nalong with a predicted version x_hat_j. These residuals are shuffled and added\nto the predicted version to recreate the variable of interest (Preserving the\ndependency between the variable of interest and the remaining variables while\nbreaking the relationship with the outcome). 2) Another option is to use the\nsampling Random Forest. Using the remaining variables to predict the variable of\ninterest, and instead of predicting the variable of interest as the mean of the\ninstances' outcome of the targeted leaf or the class with the most occurences,\nwe sample from the same leaf of the instance of interest within its neighbors,\nand we follow the standard path of the Random Forest.\n\n## References\n.. footbibliography::\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports needed for this script\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from hidimstat import BlockBasedImportance\nfrom joblib import Parallel, delayed\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom scipy.linalg import cholesky\nfrom scipy.stats import norm\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import roc_auc_score\nimport time\n\nn, p = (100, 12)\ninter_cor, intra_cor = (0, 0.85)\nn_blocks = 1\nn_signal = 2\nproblem_type = \"regression\"\nsnr = 4\nrf = RandomForestRegressor(random_state=2023)\ndict_hyper = {\"max_depth\": [2, 5, 10, 20]}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate the synthetic data\nThe function below generates the correlation matrix between the variables\naccording to the provided degrees of correlation (intra + inter). `inter_cor`\nindicates the degree of correlation between the variables/groups whereas\n`intra_cor` specifies the corresponding degree between the variables within\neach group. For the single-level case, `n_blocks` is set to 1 and the\n`intra_cor` is the unique correlation between variables.\n\nNext, we generate the synthetic data by randomly drawing n_signal predictors\nfrom the corresponding p variables and reordering the set of variables to put the\nn_signal predictors at the beginning. Following, the response is generated\nunder a simple linear model with Gaussian noise.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def generate_cor_blocks(p, inter_cor, intra_cor, n_blocks):\n    vars_per_grp = int(p / n_blocks)\n    cor_mat = np.zeros((p, p))\n    cor_mat.fill(inter_cor)\n    for i in range(n_blocks):\n        cor_mat[\n            (i * vars_per_grp) : ((i + 1) * vars_per_grp),\n            (i * vars_per_grp) : ((i + 1) * vars_per_grp),\n        ] = intra_cor\n    np.fill_diagonal(cor_mat, 1)\n    return cor_mat\n\n\ndef _generate_data(seed):\n    rng = np.random.RandomState(seed)\n\n    cor_mat = generate_cor_blocks(p, inter_cor, intra_cor, n_blocks)\n    x = norm.rvs(size=(p, n), random_state=seed)\n    c = cholesky(cor_mat, lower=True)\n    X = pd.DataFrame(np.dot(c, x).T, columns=[str(i) for i in np.arange(p)])\n\n    data = X.copy()\n\n    # Randomly draw n_signal predictors which are defined as signal predictors\n    indices_var = list(rng.choice(range(data.shape[1]), size=n_signal, replace=False))\n\n    # Reorder data matrix so that first n_signal predictors are the signal predictors\n    # List of remaining indices\n    indices_rem = [ind for ind in range(data.shape[1]) if ind not in indices_var]\n    total_indices = indices_var + indices_rem\n    # Before including the non-linear effects\n    data = data.iloc[:, total_indices]\n    data_signal = data.iloc[:, np.arange(n_signal)]\n\n    # Determine beta coefficients\n    effectset = [-0.5, -1, -2, -3, 0.5, 1, 2, 3]\n    beta = rng.choice(effectset, size=data_signal.shape[1], replace=True)\n\n    # Generate response\n    # The product of the signal predictors with the beta coefficients\n    prod_signal = np.dot(data_signal, beta)\n\n    sigma_noise = np.linalg.norm(prod_signal, ord=2) / (\n        snr * np.sqrt(data_signal.shape[0])\n    )\n    y = prod_signal + sigma_noise * rng.normal(size=prod_signal.shape[0])\n\n    return data, y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Processing across multiple permutations\nIn order to get statistical significance with p-values, we run the experiments\nacross 10 repetitions.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def compute_simulations(seed):\n    X, y = _generate_data(seed)\n    # Using the residuals\n    start_residuals = time.time()\n    bbi_residual = BlockBasedImportance(\n        estimator=\"RF\",\n        importance_estimator=\"residuals_RF\",\n        do_hypertuning=True,\n        dict_hypertuning=None,\n        conditional=True,\n        n_permutations=10,\n        n_jobs=2,\n        problem_type=\"regression\",\n        k_fold=2,\n        variables_categories={},\n    )\n    bbi_residual.fit(X, y)\n    results_bbi_residual = bbi_residual.compute_importance()\n\n    df_residuals = {}\n    df_residuals[\"method\"] = [\"residuals\"] * X.shape[1]\n    df_residuals[\"score\"] = [results_bbi_residual[\"score_R2\"]] * X.shape[1]\n    df_residuals[\"elapsed\"] = [time.time() - start_residuals] * X.shape[1]\n    df_residuals[\"importance\"] = np.ravel(results_bbi_residual[\"importance\"])\n    df_residuals[\"p-value\"] = np.ravel(results_bbi_residual[\"pval\"])\n    df_residuals[\"iteration\"] = [seed] * X.shape[1]\n    df_residuals = pd.DataFrame(df_residuals)\n\n    # Using the sampling RF\n    start_sampling = time.time()\n    bbi_sampling = BlockBasedImportance(\n        estimator=\"RF\",\n        importance_estimator=\"sampling_RF\",\n        do_hypertuning=True,\n        dict_hypertuning=None,\n        conditional=True,\n        n_permutations=10,\n        n_jobs=2,\n        problem_type=\"regression\",\n        k_fold=2,\n        variables_categories={},\n    )\n    bbi_sampling.fit(X, y)\n    results_bbi_sampling = bbi_sampling.compute_importance()\n\n    df_sampling = {}\n    df_sampling[\"method\"] = [\"sampling\"] * X.shape[1]\n    df_sampling[\"score\"] = [results_bbi_sampling[\"score_R2\"]] * X.shape[1]\n    df_sampling[\"elapsed\"] = [time.time() - start_sampling] * X.shape[1]\n    df_sampling[\"importance\"] = np.ravel(results_bbi_sampling[\"importance\"])\n    df_sampling[\"p-value\"] = np.ravel(results_bbi_sampling[\"pval\"])\n    df_sampling[\"iteration\"] = [seed] * X.shape[1]\n    df_sampling = pd.DataFrame(df_sampling)\n\n    df_final = pd.concat([df_residuals, df_sampling], axis=0)\n    return df_final\n\n\n# Running across 10 repetitions\nparallel = Parallel(n_jobs=2, verbose=1)\nfinal_result = parallel(\n    delayed(compute_simulations)(seed=seed) for seed in np.arange(1, 11)\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plotting AUC score and Type-I error\nWith the prediction problems turns to be a binary classification problem for\nthe variables being relevant or non-relevant vs the ground-truth, we measure\nthe performance in terms of type-I error i.e. the rate of true non-relevant\nvariables detected as relevant and AUC score related to correct significant\nvariables ordering.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "df_final_result = pd.concat(final_result, axis=0).reset_index(drop=True)\ndf_auc = df_final_result.groupby(by=[\"method\", \"iteration\"]).apply(\n    lambda x: roc_auc_score([1] * n_signal + [0] * (p - n_signal), -x[\"p-value\"])\n)\ndf_auc = df_auc.reset_index(name=\"auc\")\ndf_type_I = df_final_result.groupby(by=[\"method\", \"iteration\"]).apply(\n    lambda x: sum(x.iloc[n_signal:, :][\"p-value\"] <= 0.05) / x.iloc[2:, :].shape[0]\n)\ndf_type_I = df_type_I.reset_index(name=\"type-I\")\n\nauc = [\n    np.array(df_auc[\"auc\"])[: int(df_auc.shape[0] / 2)],\n    np.array(df_auc[\"auc\"])[int(df_auc.shape[0] / 2) :],\n]\ntypeI_error = [\n    np.array(df_type_I[\"type-I\"])[: int(df_type_I.shape[0] / 2)],\n    np.array(df_type_I[\"type-I\"])[int(df_type_I.shape[0] / 2) :],\n]\n\nfig, axs = plt.subplots(nrows=1, ncols=2, figsize=(8, 4), sharey=True)\n\n# AUC score\naxs[0].violinplot(auc, showmeans=False, showmedians=True, vert=False)\naxs[0].set_title(\"AUC score\")\naxs[0].xaxis.grid(True)\naxs[0].set_yticks([x + 1 for x in range(len(auc))], labels=[\"Residuals\", \"Sampling\"])\naxs[0].set_ylabel(\"Method\")\n\n# Type-I Error\naxs[1].violinplot(typeI_error, showmeans=False, showmedians=True, vert=False)\naxs[1].set_title(\"Type-I Error\")\naxs[1].axvline(x=0.05, color=\"r\", label=\"Nominal Rate\")\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Analysis of the results\nWe can observe that the sampling approaches'performance is almost similar to\nthat of the residuals. Sampling accelerates the conditional importance\ncomputation by simplifying the residuals steps.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}